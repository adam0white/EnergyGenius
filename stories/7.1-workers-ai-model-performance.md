# Story 7.1: Workers AI Model Performance Investigation & Optimization

**Epic:** Epic 7 - Post-Launch Improvements
**Status:** Ready for Development
**Priority:** P1 - Critical Performance Issue
**Complexity:** High
**Owner:** Dev Team

---

## User Story

As a platform operator, I want to optimize the recommendation AI model selection for faster inference times, so that users receive recommendations within 15-20 seconds instead of the current 45-60 second wait time.

---

## Acceptance Criteria

### Model Investigation & Research

- [ ] Use context7 MCP to research available Cloudflare Workers AI models for recommendations
- [ ] Document all available models with:
  - [ ] Model name and identifier
  - [ ] Inference speed characteristics
  - [ ] Cost implications
  - [ ] Output quality/accuracy ratings
  - [ ] Suitability for energy recommendation tasks
- [ ] Identify models that are faster alternatives to current implementation
- [ ] Create a decision matrix comparing speed vs. quality trade-offs
- [ ] Document findings in `/docs/ai-model-analysis.md`

### Performance Profiling & Benchmarking

- [ ] Set up performance monitoring for current implementation:
  - [ ] Add timing instrumentation to Workers entry point for:
    - [ ] Model initialization time
    - [ ] Prompt building time
    - [ ] Model inference time
    - [ ] Response parsing time
  - [ ] Log all timing metrics to console and response metadata
- [ ] Establish baseline metrics for current model (@cf/meta/llama-3.3-70b-instruct-fp8-fast)
- [ ] Create test harness with 5+ diverse sample inputs
- [ ] Run benchmarks and document baseline results
- [ ] Document bottleneck analysis (which phase takes longest?)

### Alternative Model Testing

- [ ] Identify 3-5 candidate faster models from context7 research
- [ ] For each candidate model:
  - [ ] Update `wrangler.toml` with test model
  - [ ] Run benchmark test harness
  - [ ] Measure inference time improvement
  - [ ] Evaluate output quality (compare recommendation sensibility)
  - [ ] Document results with timing and quality assessment
- [ ] Select optimal model balancing speed and quality
- [ ] Document selection rationale in `/docs/ai-model-optimization.md`

### Implementation & Validation

- [ ] Update `wrangler.toml` to use optimized model:
  - [ ] Change `AI_MODEL_FAST` to selected faster model
  - [ ] Keep `AI_MODEL_ACCURATE` as fallback (retain current model)
  - [ ] Document model choices in configuration
- [ ] Verify no regression in output quality:
  - [ ] Test with all 5+ sample inputs
  - [ ] Verify "Why" explanations are sensible
  - [ ] Verify recommendation values are reasonable
  - [ ] Check for any API errors or timeouts
- [ ] Deploy optimized configuration to staging
- [ ] Run final performance test:
  - [ ] Target response time: < 25 seconds end-to-end
  - [ ] Verify progress updates still show at 10s
  - [ ] Confirm no timeout errors
- [ ] Document performance improvement metrics

### Code & Documentation

- [ ] Add performance monitoring to `/src/worker/index.ts`:
  - [ ] Timing instrumentation for each phase
  - [ ] Metadata in API response with timing breakdown
  - [ ] Console logging for debugging
- [ ] Update `/docs/architecture.md`:
  - [ ] Document AI model selection rationale
  - [ ] Add performance characteristics section
  - [ ] Include model comparison results
  - [ ] Document fallback model strategy
- [ ] Update wrangler.toml comments:
  - [ ] Note performance characteristics of selected models
  - [ ] Explain fallback strategy
- [ ] Commit with message: "Optimize Workers AI model for faster inference"

## Tasks / Subtasks

- [ ] Research Models with context7 MCP (AC: Model Investigation & Research)
  - [ ] Set up context7 MCP integration
  - [ ] Query available models
  - [ ] Document findings
  - [ ] Create decision matrix
- [ ] Set up Performance Profiling (AC: Performance Profiling & Benchmarking)
  - [ ] Add timing instrumentation to worker
  - [ ] Create benchmark test harness
  - [ ] Run baseline benchmarks
  - [ ] Document bottleneck analysis
- [ ] Test Alternative Models (AC: Alternative Model Testing)
  - [ ] Test 3-5 candidate models
  - [ ] Measure timing for each
  - [ ] Evaluate output quality
  - [ ] Select optimal model
- [ ] Implement & Deploy (AC: Implementation & Validation)
  - [ ] Update wrangler.toml
  - [ ] Verify quality and performance
  - [ ] Deploy to staging
  - [ ] Run final validation
- [ ] Document Changes (AC: Code & Documentation)
  - [ ] Add performance monitoring code
  - [ ] Update architecture documentation
  - [ ] Commit changes

## Dev Notes

- The recommendation takes ~45-60s total, but progress UI shows success at ~10s then waits
- Current model: `@cf/meta/llama-3.3-70b-instruct-fp8-fast`
- This suggests either:
  - Model inference itself is slow (30-50s)
  - Token generation is streaming slowly
  - Multiple inferences happening in sequence
- Context7 MCP can be used to investigate available models and their performance characteristics
- Priority: Find faster models that maintain recommendation quality
- Consider trade-off: Speed vs. accuracy (fallback model strategy recommended)

### Project Structure Notes

- Worker entry point: `/src/worker/index.ts`
- Model configuration: `/wrangler.toml`
- Architecture docs: `/docs/architecture.md`
- AI pipeline orchestration: `/src/worker/ai/orchestration.ts` [Source: docs/epic-3-ai-pipeline.md]
- Consider: Context7 MCP for live model lookup vs. static wrangler.toml config

### References

- [Workers AI Documentation](https://developers.cloudflare.com/workers-ai/)
- [Source: docs/epic-3-ai-pipeline.md#AI Pipeline Orchestration]
- [Source: wrangler.toml - AI Configuration]
- Context7 MCP for available models research

## Dev Agent Record

### Context Reference

<!-- Path(s) to story context XML will be added here by context workflow -->

### Agent Model Used

Claude Haiku 4.5

### Debug Log References

### Completion Notes List

### File List
