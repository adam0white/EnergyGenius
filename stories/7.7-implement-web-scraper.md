# Story 7.7: Implement Web Scraper for Real Plans

**Epic:** Epic 7 - Post-Launch Improvements
**Status:** Ready for Development
**Priority:** P1 - High
**Complexity:** High
**Owner:** Dev Team

---

## User Story

As a developer, I need a working web scraper that retrieves real energy plans from actual supplier websites, so the application can provide real plan recommendations instead of relying solely on mock data.

---

## Acceptance Criteria

### 1. Investigate Current Scraper Implementation

- [ ] Locate existing scraper code:
  - [ ] Where is the scraper implemented? (likely `/src/worker/scraper/` or `/src/scripts/scraper/`)
  - [ ] What is the current state of the scraper?
  - [ ] What is being scraped currently?
  - [ ] Why is it not working?

- [ ] Document current issues:
  - [ ] What errors occur when running the scraper?
  - [ ] Are there network issues, parsing issues, or data structure issues?
  - [ ] Are target websites blocking the scraper?
  - [ ] Is the scraper timing out?
  - [ ] Does it parse data correctly but fail to save?

- [ ] Understand the target websites:
  - [ ] Which supplier websites need to be scraped?
  - [ ] What is the data structure on each site?
  - [ ] Are there terms of service considerations?
  - [ ] Do sites have robots.txt or anti-scraping measures?
  - [ ] Are there better data sources (APIs, public feeds)?

- [ ] Review scraper requirements:
  - [ ] What data needs to be extracted? (name, price, renewable %, tier, features, etc.)
  - [ ] How often should scraper run? (daily, weekly?)
  - [ ] Where should scraped data be stored?
  - [ ] How should it integrate with recommendation system?

- [ ] Developer has full authority to investigate and understand the entire scraper system

### 2. Fix Scraper Implementation

Based on investigation, fix the scraper:

**Option A: Fix Existing Scraper**
- [ ] If scraper exists but has bugs:
  - [ ] Debug parsing logic
  - [ ] Fix data extraction
  - [ ] Handle website structure changes
  - [ ] Add error handling
  - [ ] Fix data storage logic

**Option B: Rebuild Scraper**
- [ ] If scraper is fundamentally broken:
  - [ ] Choose scraper library (axios + cheerio, puppeteer, playwright, etc.)
  - [ ] Implement scraper for each target website
  - [ ] Handle site-specific parsing logic
  - [ ] Add retry logic for network failures
  - [ ] Add data validation

**Option C: Use Alternative Data Source**
- [ ] If website scraping is not viable:
  - [ ] Research energy plan APIs
  - [ ] Look for public datasets
  - [ ] Consider partnerships with suppliers
  - [ ] Document why scraping wasn't viable

### 3. Scraper Implementation Details

- [ ] For each target supplier website:
  - [ ] Identify data elements to extract:
    - [ ] Plan name
    - [ ] Price per kWh
    - [ ] Renewable percentage
    - [ ] Contract terms (months)
    - [ ] Features/benefits
    - [ ] URL for more info
  - [ ] Write parser for that website
  - [ ] Test parser with actual website
  - [ ] Handle data format variations

- [ ] Implement robust scraping:
  - [ ] Add retry logic for network failures (exponential backoff)
  - [ ] Add rate limiting to be respectful to servers
  - [ ] Add user-agent rotation if needed
  - [ ] Add proxy support if needed
  - [ ] Add proxy monitoring and error handling
  - [ ] Add data validation for extracted information
  - [ ] Add logging for debugging scraper runs

- [ ] Data standardization:
  - [ ] Normalize plan names across suppliers
  - [ ] Convert prices to consistent format
  - [ ] Standardize renewable percentages (ensure 0-100)
  - [ ] Standardize tier classifications
  - [ ] Standardize contract terms

### 4. Store and Integrate Scraped Data

- [ ] Design data storage:
  - [ ] Where to store scraped plans? (JSON file, database, cache?)
  - [ ] File location: `/src/worker/data/scraped-plans.ts` or equivalent
  - [ ] Include timestamp of when data was scraped
  - [ ] Include source website for each plan

- [ ] Integrate with recommendation system:
  - [ ] Update recommendation handler to use scraped data
  - [ ] Combine scraped data with mock data if needed
  - [ ] Ensure scraped data is available to Claude prompts
  - [ ] Validate scraped data before using in recommendations

- [ ] Create scraper runner:
  - [ ] Script to run scraper: `/src/scripts/run-scraper.ts`
  - [ ] Can be run manually: `npm run scraper`
  - [ ] Or scheduled automatically (if server-based)
  - [ ] Outputs status and any errors

### 5. Testing the Scraper

- [ ] Unit tests:
  - [ ] Create `/test/scraper.spec.ts`
  - [ ] Test parsing logic with example HTML
  - [ ] Test data validation
  - [ ] Test error handling
  - [ ] Test data normalization

- [ ] Integration tests:
  - [ ] Test scraper with actual websites (use appropriate waits)
  - [ ] Verify data is stored correctly
  - [ ] Verify data can be loaded and used
  - [ ] Test with different scenarios (network errors, parsing errors, etc.)

- [ ] Manual testing:
  - [ ] Run scraper against real websites
  - [ ] Verify all plans are captured
  - [ ] Verify data quality
  - [ ] Verify recommendations use scraped data correctly
  - [ ] Generate recommendations and verify accuracy

### 6. Documentation and Monitoring

- [ ] Document the scraper:
  - [ ] Create `/docs/scraper.md`:
    - [ ] How to run the scraper
    - [ ] What websites are being scraped
    - [ ] How often to run it
    - [ ] How to troubleshoot scraper issues
    - [ ] Data storage and format
  - [ ] Document each website's parser
  - [ ] Document data validation rules
  - [ ] Document how scraped data integrates with recommendations

- [ ] Add monitoring/logging:
  - [ ] Log scraper startup and completion
  - [ ] Log number of plans scraped per supplier
  - [ ] Log any parsing errors or data validation failures
  - [ ] Log timestamp of last successful scrape
  - [ ] Add error alerts for failed scraper runs

- [ ] Update README:
  - [ ] Document how to setup and run scraper
  - [ ] Document data freshness expectations
  - [ ] Document fallback to mock data if scraper fails

- [ ] Commit message: "Implement web scraper for real energy plan data"

## Tasks / Subtasks

- [ ] Investigate Scraper (AC: Investigate Current Scraper Implementation)
  - [ ] Find scraper code
  - [ ] Document issues and failures
  - [ ] Identify target websites
  - [ ] Understand requirements

- [ ] Fix/Rebuild Scraper (AC: Fix Scraper Implementation)
  - [ ] Debug or rebuild scraper
  - [ ] Handle all target websites
  - [ ] Add robust error handling
  - [ ] Add data validation

- [ ] Implement Details (AC: Scraper Implementation Details)
  - [ ] Extract required data fields
  - [ ] Add retry and rate limiting
  - [ ] Normalize data formats
  - [ ] Document parsing logic

- [ ] Store & Integrate (AC: Store and Integrate Scraped Data)
  - [ ] Design data storage
  - [ ] Create storage structure
  - [ ] Integrate with recommendations
  - [ ] Create scraper runner script

- [ ] Test Scraper (AC: Testing the Scraper)
  - [ ] Write unit tests
  - [ ] Write integration tests
  - [ ] Test with real websites
  - [ ] Verify recommendations work

- [ ] Document (AC: Documentation and Monitoring)
  - [ ] Write scraper documentation
  - [ ] Add logging and monitoring
  - [ ] Update README
  - [ ] Commit changes

## Dev Notes

### Investigation Authority

Developer has full authority to:
- Analyze target websites and data structure
- Choose scraper technology and libraries
- Modify scraper implementation
- Change data storage approach
- Modify recommendation integration
- Research alternative data sources
- Add dependencies for scraping

### Scraper Approaches

**Lightweight Option:**
- Use: axios + cheerio (smaller, simpler)
- Pros: Fast, low overhead, good for simple HTML
- Cons: No JavaScript rendering
- Use when: Target sites are static HTML

**Full Browser Option:**
- Use: Puppeteer or Playwright
- Pros: Handles JavaScript, dynamic content
- Cons: Heavier, slower, more resources
- Use when: Sites use JavaScript to load content

### Legal/Ethical Considerations

- Check website terms of service
- Respect robots.txt
- Add rate limiting and delays
- Identify your scraper with proper user-agent
- Don't overload servers
- Consider reaching out to suppliers for data access
- Consider if there are APIs available instead

### Example Scraper Structure

```typescript
// /src/worker/scraper/index.ts
interface ScrapedPlan {
  supplier: string;
  name: string;
  pricePerKwh: number;
  renewablePercentage: number;
  contractMonths: number;
  features: string[];
  sourceUrl: string;
  scrapedAt: Date;
}

async function scrapePlans(): Promise<ScrapedPlan[]> {
  const plans: ScrapedPlan[] = [];

  for (const supplier of targetSuppliers) {
    try {
      const supplierPlans = await scrapeSupplier(supplier);
      plans.push(...supplierPlans);
    } catch (error) {
      console.error(`Failed to scrape ${supplier}:`, error);
      // Continue with other suppliers
    }
  }

  return plans;
}
```

### Common Scraping Issues

1. **Website Changes**: Parser breaks when site changes
   - Solution: Regular monitoring and updates

2. **Rate Limiting**: Website blocks frequent requests
   - Solution: Add delays between requests, rotate IPs if needed

3. **Dynamic Content**: Site uses JavaScript to load data
   - Solution: Use Puppeteer/Playwright instead of cheerio

4. **Data Quality**: Scraped data is inconsistent or incomplete
   - Solution: Add validation and normalization

5. **Legal Issues**: Scraping violates terms of service
   - Solution: Research APIs or contact suppliers

### Storage Considerations

Store scraped data in a file that can be:
- Easily loaded by recommendation system
- Updated without restarting app
- Compared to previous scrapes
- Validated for quality
- Backed up

Example storage:
```typescript
// /src/worker/data/scraped-plans.ts
export interface ScrapedDataSnapshot {
  timestamp: string;
  source: 'web-scraper';
  suppliers: {
    [supplierName: string]: ScrapedPlan[];
  };
}

export const scraped_plans: ScrapedDataSnapshot = {
  // ... loaded from scraper
};
```

## Dev Agent Record

### Context Reference

<!-- Story context XML will be added here by context workflow -->

### Agent Model Used

Claude Sonnet 4.5

### Debug Log References

### Completion Notes List

<!-- To be filled in during development -->

## QA Checklist

- [ ] Scraper runs without errors
- [ ] All target suppliers are being scraped
- [ ] Plans are extracted with all required fields
- [ ] Data is validated and normalized
- [ ] Scraped data is stored successfully
- [ ] Recommendations use scraped data
- [ ] Manual testing shows accurate data
- [ ] Error handling works for failures
- [ ] Logging captures scraper activity
- [ ] Documentation is complete
- [ ] Tests pass (unit and integration)
- [ ] Changes committed
